---
title: "알파고의 역사"
categories: 
- 공학작문및발표
- AI
tags:
- AI
- GraphViz
toc: true   #Table Of Contents 목차 
use_math: true
toc_sticky: true
---

## 개요

공학작문 및 발표(4학년 2학기)시간에서 GraphViz를 배우고 이를 이용한 보고서 작성을 하였다. 알파고는 트리 구조로 생각할 수 있고 알파고의 학습 방법을 트리를 이용하여 설명했다. 여기서 트리는 GraphViz와 Python을 활용해서 그렸다. 해당 코드는 다음과 같다.   

```python
import graphviz as gv
import PIL.Image as pil
import numpy as np


g1 = gv.Digraph(format='jpg')

# --------- 1 단계 --------

g1.node("node0", shape='plaintext', label = "", image = "./Figures/node0.png")
g1.node("node1", shape='plaintext', label = "", image = "./Figures/node1.png")
g1.node("node2", shape='plaintext', label = "", labelangle = "max", image = "./Figures/node2.png")
g1.node("node3", shape='plaintext', label = "", image = "./Figures/node3.png")
g1.node("white", shape='plaintext', label = "", image = "./Figures/white.png")
g1.edge( "node0", "node1", taillabel="Q + u(P)  ", label="  max", style = "dotted")
g1.edge( "node0", "node2", taillabel="  Q + u(P)", style = "bold")
g1.edge( "node2", "white", taillabel="  Q + u(P)", style = "bold")
g1.edge( "node2", "node3", taillabel="Q + u(P)  ", label="  max",style = "dotted")



filename1 = g1.render(filename='Ralo-Step1')
```

위 코드와 같은 방식으로 트리를 만들고 그림을 생성하여 보고서를 작성하였다. 해당 보고서를 바탕으로 포스팅을 진행한다.

## 알파고의 탄생 배경

알파고의 탄생 배경에 대해 알아본다. 이와 관련해서 개발자와 알파고를 간단히 소개한다. 그 후, 이세돌과 대결했던 당시 알파고의 성능에 대해 알아본다. 

### 개발자 소개

알파고를 개발한 ‘데미스 하사비스(Demis Hassabis)’는 구글 딥마인드(Deepmind)의 창업자이자 최고경영자이다. 13살의 나이에 체스 마스터의 자리에 올랐고, 17살에 게임 개발에 나섰다. 그 후, 28살에 게임 업계에서 은퇴한 후 인공지능 개발에 뛰어들었다. 인공지능을 개발하려면 먼저 사람의 뇌에 대한 이해가 필요하다는 판단을 하고 유니버시티 칼리지 런던에 들어가 인지신경과학(뇌과학)을 연구하기 시작했다. 이때 그는 기억과 상상이 뇌의 같은 부분에서 생겨난다는 것을 발견했다. ‘사이언스’지는 이 발견을 2007년 세계 10대 과학 성과 가운데 하나로 꼽았다.

2009년 뇌과학 박사 학위를 취득한 하사비스는 2010년 인공지능 스타트업 딥마인드를 설립했다. 2014년 딥마인드는 차세대 성장동력을 찾던 구글에 인수되었고, 이와 함께 하사비스는 인공지능 부문 부사장으로 구글에 합류했고, 2015년 계열사 분리에 맞춰 구글 딥마인드의 최고경영자가 되었다.

### 알파고의 간단한 소개

하사비스는 원래 게임 개발자였다. 게임을 개발하면서 게임이야말로 가장 창의적인 작업이라는 것을 깨달은 하사비스는 인공지능이 사람과 대등해지려면 먼저 게임부터 정복해야 한다는 사실을 깨달았다.

한편, 바둑은 막대한 서치 공간문제와 함께 돌이 지닌 위치의 가치(board position) 및 움직임을 평가하기 어렵다. 따라서 체스와 같은 다른 게임에 비해 인공지능 분야에서 해결하기에 가장 어려운 고전 게임으로 여겨져 왔다. 

이러한 과정이 맞물려서 구글의 지주회사 ‘알파벳’에 바둑을 의미하는 영어단어‘고(Go)’가 합쳐져 구글의 바둑 인공지능을 의미하는 알파고가 태어났다. 

### 알파고의 성능

2016년 당시 알파고 컴퓨터는 CPU 1202개, GPU 176개를 쓴 네트워크 컴퓨터(분산형 컴퓨터)였다. 컴퓨터 하드웨어 성능만 보면 세계 500대 슈퍼컴퓨터 순위 하위권에 든다. 하사비스에 따르면 “하드웨어 성능을 개선하면 오히려 알파고의 성능이 떨어져 하드웨어 용량을 늘리지 않았다”라면서 “알고리즘만 개선해 이세돌 9단과의 대국을 준비해왔다”라고 말했다. 

이는 분산형 컴퓨터가 분산 처리할 때 CPU 개수가 늘어나면 통신에 과부하가 걸려 알파고의 성능이 오히려 떨어졌기 때문이다. IBM의 슈퍼컴퓨터 ‘딥블루’가 1997년 세계 체스 챔피언 게리 카스파로프를 이겼을 때는 하드웨어(컴퓨터) 성능 개선이 승리 요인이었는데, 알파고는 하드웨어보다는 소프트웨어인 알고리즘 개선으로 인간에 도전했다는 점이 주목해 볼 만한 요인으로 여겨진다.

## 알파고의 학습 방법

알파고의 학습 방법에 대해 알아본다. 알파고는 두 가지 핵심 요소를 지닌다. 첫째로는 ‘가치망’을 통해 트리의 깊이를 제한하면서 현재 대국 상황의 승산을 나타낸다. 승산이 높을수록 많은 수를 볼 필요가 없다. 가치 네트워크는 바둑의 전체적인 형세를 파악해 현재 승률을 추정한다. 둘째로는 ‘정책망’을 통해서 트리의 폭을 제한한다. 확장 과정에서 주로 사용되는데, 측정 시점에서 가능한 모든 수 가운데 가장 승률이 높은 것을 예측한다. 이는 바둑기사의 착수를 결정하는 것이다. 이 두 딥 뉴럴 네트워크는 프로기사의 기보를 통한 교사학습과 스스로 플레이를 통한 강화학습이라는 새로운 조합에 의해서 학습된다. 또한, ‘가치망’과 ‘정책망’에 몬테카를로 방식을 조합하여 다음과 같은 과정을 밟게 된다.

> __몬테카를로 방식이란?__
>
> 몬테카를로 방식은 원하는 결괏값을 정확한 값을 얻는 방법이 아니고, 난수를 이용하여 어떤 함수의 답을 확률적으로 근접하게 계산하는 방식이다. 이 알고리즘을 활용하는 대표적인 예로 원주율 π 값을 구하는 것이 있다.

### 선택(Selection)

첫째로, 선택 단계이다.

![alphago1](https://user-images.githubusercontent.com/48538655/103111041-ecab9b80-468b-11eb-8f14-af079708f19a.JPG){: .align-center}

각각 시뮬레이션은 액션 값 Q와 보너스값  u(P)의 합이 가장 큰 에지를 선택하여 트리를 따라 이동한다. 여기서 Q 값은 몬테카를로의 가치 값 등으로 정해진 것으로 높을수록 승리할 확률이 높다. 또한, u 값은 바둑판 탐색의 폭을 넓히기 위해서 고안된 변수로 에지에 저장된 사전확률 P의 함수이다. 이 과정은 바둑에서 수읽기를 진행하는 부분이다.

### 확장(Expansion)

다음으로, 확장 단계이다.

![alphago2](https://user-images.githubusercontent.com/48538655/103111088-8d01c000-468c-11eb-98f6-21b1b0cdef48.JPG){: .align-center}

특징 시점까지 선택이 된 노드로부터 확장(자식 노드 생성)을 수행하게 되는데, 알파고에서 확장하는 기준은 마지막 노드의 방문 횟수가 40회 이상인 경우가 된다. 이 과정은 바둑에서 수읽기를 확장하는 부분이다.

### 평가(Evaluation)

다음으로, 평가 단계이다.

![alphago3](https://user-images.githubusercontent.com/48538655/103111127-e2d66800-468c-11eb-897d-08a7eb18ca13.JPG){: .align-center}

마지막 노드의 가치를 평가하기 위해서 마지막 노드 시점부터 게임 종료까지 고속 시뮬레이션(fast rollout)을 수행한다. 시뮬레이션의 평균값과 딥러닝으로 추정한 가치 값을 통해서 마지막 노드의 가치를 평가한다.

### 갱신(Backup)

다음으로, 갱신 단계이다.

![alphago4](https://user-images.githubusercontent.com/48538655/103111139-026d9080-468d-11eb-97e5-ff2a8ebb7bf1.JPG){: .align-center}

참고로 위 그림에서 각 박스는 Q 값을 갱신하는 범위를 의미한다. 시작 지점에서 마지막 노드까지의 경로에 있는 노드의 Q 값을 갱신하게 된다. 이렇게 4가지 단계를 계속 반복하면서 최적의 경로를 찾아내게 된다.

## 알파고의 발전 과정

첫째로 나온 알파고 판(AlphaGo Fan)은 176개의 GPU가 사용된 분산 버전이다. 이는 지난 2015년 10월 천재 바둑기사 판 후이(Fan Hui) 2단을 이기고 2016년 네이처(Nature)에 실린 버전이다. 다음으로 나온 알파고 리(AlphaGo Lee)는 48개의 TPU가 사용된 분산 버전으로 2016년 3월 이세돌 9단과의 대국에서 승리하였다.

다음으로 알파고 마스터(AlphaGo Master)이다. 4개의 TPU가 사용된 단일 버전이다. 2017년 초 프로 바둑기사와 온라인 대국에서 60연승을 하였고, 같은 해 커제 9단과의 대결에서도 승리하였다. 구글의 설명에 따르면 사용된 TPU의 연산 성능은 당신의 최신 CPU보다 30~80배 높다고 설명하였다. 단일 버전인 알파고 마스터의 연산 능력은 분산 버전의 10% 수준이지만, 주 기능을 인공지능 연산과 예측에만 특화해 기계학습 알고리즘과 텐서플로(TensorFlow), 프레임워크(framework) 구동에 최적화되었다. 기존의 알파고가 학습한 내용을 토대로 추론했다면 알파고 마스터는 추론과 동시에 학습할 수 있고, 학습에 필요한 시간이 기존의 3분의 1로 단축되었다. 또한, 기계의 물리적인 부피가 줄어들면서 에너지 효율은 10배가량 향상되었다.

다음으로 알파고 제로(AlphaGo Zero)이다. 4개의 TPU가 사용된 단일 버전으로 알파고의 최종 버전이다. 알파고 제로는 인간의 기보에 의존하는 지도학습 없이 바둑 규칙만으로 스스로 학습하며 기력을 향상한다. 학습 36시간 만에 알파고 리의 수준을 능가하였고, 72시간 만에 알파고 리와 대국에서 100승 하는 동안 패하지 않았으며, 40일 후 알파고 마스터와 대국에서는 89승 11패를 기록하였다. 빅데이터 학습이 필요 없는 인공지능의 등장은 바둑과 달리 빅데이터 확보가 어려워 인공지능을 활용하기 어려웠던 분야에 해결책을 제시했다는 점에서 의미가 있다.

> __GPU란?__
>
> GPU는 컴퓨터 시스템에서, 그래픽 연산을 빠르게 처리하여 결괏값을 모니터에 출력하는 연산장치이다.

> __TPU란?__
>
> TPU(Tensor Processing Unit)는 구글에서 2016년 5월에 발표한 데이터 분석 및 딥러닝용 하드웨어이다. 벡터/행렬연산의 병렬처리에 특화되어 있다.

## 마무리

이처럼 알파고에 대해 알아보았다. 알파고는 인류에게 큰 충격을 안겨주었으며, 인공지능에 대해 잘 모르던 사람들까지 인공지능이 얼마나 발전했는지 알려주었다. 

한편, 알파고는 커제 9단과의 대결을 끝으로 은퇴를 선언했다. 세계 정상 기사들과의 대국을 통해 희망했던 정점에 도달했다는 점이 그 이유이다. 이에 따라 하사비스 CEO는 “알파고 연구팀은 앞으로 과학자들이 암 등 질병 치료, 에너지 소모 감축, 혁신적인 신소재를 찾는 등 더 크고 복잡한 과제를 해결하는 것을 돕는데, 모든 힘을 쏟아부을 것”이라고 말했다. 알파고가 더는 바둑에 특화된 AI가 아닌 범용 AI로 진화 발전시키겠다는 계획이다. 앞으로 알파고를 비롯한 여러 AI의 발전을 기대해본다.

​    