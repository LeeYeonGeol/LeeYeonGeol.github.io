---
title: "[Project] 표정인식을 통한 이모티콘 생성기"
categories: 
- Project
tags:
- Deep Learning
- CNN
toc: true   #Table Of Contents 목차 
toc_sticky: true
---
## 개요

4학년 졸업과제의 일환으로 진행하였다. 사진 또는 웹캠을 입력받아 얼굴인식 후 표정에 따른 감정과 안경, 마스크 착용 상태, 인종 등을 반영한 이모티콘을 출력하는 웹 기반 프로그램이다. 나는 데이터 전처리 및 CNN 알고리즘을 활용한 얼굴인식 모델링을 맡아서 진행하였다.

## 구현 과정

### 데이터 수집 및 전처리

안경과 관련된 데이터셋과 마스크와 관련된 데이터셋의 경우, 무료로 이용할 수 있는 kaggle 데이터셋을 다운받아서 사용하였다. __하지만, 대부분의 데이터셋이 백인 혹은 동양인으로 이루어져 있어서 피부색을 판단하는데 필요한 데이터가 현저히 부족하였다.__ 이는 직접 검색을 통해 이미지를 찾고 라벨링을 하였는데 그래도 부족해서 data augmentation기법을 활용해서 좌우 반전을 하거나 각도를 조금 비틀어주는 등 데이터를 증가시켰다. 다음은 이를 사용한 예시이다.

<p align = "center"><img src="https://user-images.githubusercontent.com/48538655/103834815-dcea7900-50c7-11eb-8b0b-b3e3bcd10442.JPG" alt="졸과1" style="zoom:100%;" /></p>

한편, 얼굴인식을 진행하고 그에 대한 모델을 적용할 것이므로 모델링에 사용된 데이터 또한 얼굴인식 알고리즘(caffemodel)을 활용하여 데이터 전처리를 진행하였다.

<p align='center'>
    <img src="https://user-images.githubusercontent.com/48538655/103834816-de1ba600-50c7-11eb-9849-37d4aabb7dd4.JPG" alt="졸과2" style="zoom:100%;" />
</p>

### 딥러닝

감정 인식 같은 경우 데이터셋을 직접 모으기 힘들기 때문에 오픈소스를 활용하여 학습을 진행하였다. 

다음은 학습 정확도와 손실 그래프이다. ([출처]( https://github.com/atulapra/Emotion-detection))

<p align='center'>
    <img src="https://user-images.githubusercontent.com/48538655/103834817-de1ba600-50c7-11eb-9a35-b6db7895536c.JPG" alt="졸과3" style="zoom:100%;" />
</p>

지금와서 생각해보면 그래프에서도 알 수 있듯이 오버피팅된 자료인데 이를 파라미터를 조정한다던가 드롭아웃 혹은 배치 정규화를 사용, 조정한다던가 여러가지 공개된 모델(AlexNet, VGGNet, GoogleNet 등)을 활용해서 더 좋은 정확도를 얻을 수 있었을 것같은데 시도를 해보지 않았던게 아쉽다. 

한편 안경 및 마스크 인식의 경우 다음과 같이 진행하였다.

<p align = 'center'>
    <img src="https://user-images.githubusercontent.com/48538655/103838017-5b96e480-50cf-11eb-9e7b-8c8bc6e93ca1.JPG" alt="졸과4" style="zoom:100%;" />
</p>

사이즈가 128x128이고 채널이 3인 이미지를 인풋으로 받고 이미지의 특성을 반영하기 위한 Convolution 계층과 이미지 해상도를 줄이기 위한 Maxpooling을 교차로 사용하면서 오버피팅을 방지하기 위하여 Dropout도 중간에 삽입하였다.

### 정확도 개선

#### 배치 사이즈 조정

한편 정확도 개선을 위해서 다음과 같은 작업을 진행하였다. 우선, 배치 사이즈를 조정하였는데 결과는 다음과 같았다.

| 배치 사이즈 | 1 에폭당 훈련 횟수 | 1 에폭당 평균 걸리는 시간 | 정확도 |
| ----------: | -----------------: | ------------------------: | -----: |
|           1 |              506회 |                약 40~50초 | 76~82% |
|         100 |                5회 |                   약 20초 | 82~84% |

배치 사이즈가 작다면 1 에폭당 훈련 횟수가 늘어나고 그에 따라 총 훈련 횟수가 늘어난다. 따라서, 훈련 시간이 늘어났다. 따라서 실험에서는 배치 크기가 클 때가 보다 빠르게 최적의 성능에 도달할 수 있다는 이유로 배치 크기는 클수록 좋아고 추론을 하였다.

하지만, 다음의 그래프를 보자. (배치 크기에 따른 모델링 과정 ([출처](https://www.kakaobrain.com/blog/113)))

<p align="center">
    <img src="https://user-images.githubusercontent.com/48538655/103838718-1d022980-50d1-11eb-9087-13a09c8294cd.JPG" alt="졸과5" style="zoom:100%;" />
</p>

최소값임을 보장할 수 없는 극솟값 또는 안장점 근처(local minima)에서 학습이 진행된다고 가정하자. 배치크기가 큰 상황에서는 훈련 횟수가 적으므로 이 구간을 빠져나오기가 어렵다. 반면, 배치 크기가 작다면 이 구간에서 빠져나오기가 훨씬 수월하다.

__이에 따라 적절한 배치 크기를 정해주는 것이 매우 중요하다는 결론을 내었다.__

#### 최적화 함수 조정

다음으로는 최적화 함수를 다르게 주었다. (Optimizer 발전 과정 ([출처](https://gomguard.tistory.com/187)))

<p align='center'>
    <img src="https://user-images.githubusercontent.com/48538655/103839163-1d4ef480-50d2-11eb-884a-c0f837b77743.JPG" alt="졸과6" style="zoom:120%;" />
</p>

우선, Momentum의 개념에 대해서 알아보자. Gradient dexcent기반의 알고리즘으로 수식으로 표현하면 다음과 같다.

![졸과7](https://user-images.githubusercontent.com/48538655/103839311-76b72380-50d2-11eb-8b9f-891031b42c74.JPG){: .align-center}

가중치를 더해줄 때, v를 더해주게 되는데 이는 일종의 가속도(혹은 속도)같은 개념으로 가중치가 감소하던(혹은 증가하던)방향으로 더 많이 변화할 수 있도록 해준다.

다음은 기존 SGD함수를 사용했을 때와 Momentum개념이 들어간 Adam함수를 사용했을 때 의 차이를 보여준다.

<p align='center'>
    <img src="https://user-images.githubusercontent.com/48538655/103840011-01e4e900-50d4-11eb-89c0-969ac64a5fd6.JPG" alt="졸과8" style="zoom:80%;" />
</p>

위를 통해 Momentum개념이 들어간 Adam함수가 학습이 증가하는 방향으로 기울기가 더 큰 것을 확인할 수 있다. 

이제 Adagrad함수를 알아본다. 기존에서는 모든 가중치에 동일한 학습률을 적용해 왔다. 하지만 직관적으로 생각해보면 어떤 가중치는 빠르게 학습이 끝날 수도 있고 다른 경우에서는 더 큰 학습률 적용이 필요한 경우도 있다. 따라서 고안된 것이 Adagrad함수이다. 과거의 기울기를 제곱하여 계속 더하면서 빈번히 업데이트가 일어나는 가중치의 학습률은 작게 하고 반대의 경우는 크게 하는 방법이다.

하지만 Adagrad함수는 과거의 기울기를 제곱하여 계속 더해가므로 학습을 진행할수록 갱신 강도가 약해진다. 실제로 무한히 계속 학습한다면 어느 순간 갱신량이 0이 되어 전혀 갱신되지 않게 된다. 

이 문제를 개선한 기법으로 RMSProp함수가 있다. RMSProp함수는 과거의 기울기를 균일하게 더해가는 것이 아니라, 먼 과거의 기울기는 서서히 잊고 새로운 기울기 정보를 크게 반영한다. 이를 지수이동평균(Exponential Moving Average, EMA)라고 하며 과거 기울기의 반영 규모를 기하급수적으로 감소시킨다. 다음의 그래프를 보자.

<p align='center'>
    <img src="https://user-images.githubusercontent.com/48538655/103840010-00b3bc00-50d4-11eb-8240-35216e6e6986.JPG" alt="졸과9" style="zoom:80%;" />
</p>

위의 그래프를 보면 Adagrad함수는 어느 순간부터 전혀 갱신이 되지 않음을 확인할 수 있고 RMSProp함수는 그 문제가 개선이된 것을 확인할 수 있다.

__이러한 비교를 통해서 Momentum과 Adaptive learning rate를 함께 반영한 Adam함수의 extension인 Adamax함수를 주로 이용하여 구현하게 되었다.__

## 결과

다음은 각 상황에 따른 출력되는 이모티콘들이다. 황인, 흑인, 백인으로 구분하였고 감정과 안경, 선글라스, 마스크에 따라 구분하였다.

<p align='center'>
    <img src="https://user-images.githubusercontent.com/48538655/103841683-7bcaa180-50d7-11eb-9542-e075aa055073.JPG" alt="졸과11" style="zoom:80%;" />
</p>

모든 모델링을 끝마친 뒤, 16장의 사진 파일로 위에서 설정한 이모티콘이 잘 출력되는지 확인해 보았다.

<p align='center'>
    <img src="https://user-images.githubusercontent.com/48538655/103841444-f3e49780-50d6-11eb-9468-f101de137a8a.JPG" alt="졸과10" style="zoom:110%;" />
</p>

위 결과를 바탕으로 정리해보면 마스크, 선글라스 착용과 피부색에 대해서는 높은 적중률을 확인할 수 있었다. 다만 아쉬웠던 점이 있다면 수염에 대한 데이터셋이 고려되지 않아서 수염이 있을 시 흑인으로 판단하는 케이스가 있었다. 또한 동그란 안경과 네모난 안경을 구분하려고 모델링을 했으나 두 경우에 차이점이 모호해서 만족스럽지 못한 결과가 나왔다.

## 느낀점

졸업과제를 통해 프로젝트를 경험해볼 수 있어서 좋았고 특히 딥러닝을 처음 접해보았는데 관심이 생기는 계기가 되었다. 그리고 열심히 노력한 결과 우수상을 받을 수 있었다.

<p align='center'>
    <img src="https://user-images.githubusercontent.com/48538655/103842448-4aeb6c00-50d9-11eb-84e0-79b05538e83f.jpg" alt="졸과10" style="zoom:50%;" />
</p>

다만 아쉬운 점은 파라미터 조정을 통해서 좀 더 정확도 향상을 할 수 있었지 않을까...싶다. 뜻깊은 경험이었다. 

## 추가 자료

관련 링크를 첨부한다.

> [데이터셋(깃허브)](https://github.com/LeeYeonGeol/Make_Emotion_OpenCV/tree/dependabot/pip/tensorflow-2.3.1/final_data)
>
> [코드(깃허브)](https://github.com/LeeYeonGeol/Make_Emotion_OpenCV)
>
> [시연동영상(유튜브)](https://youtu.be/Y4PT3xLeZAw)

