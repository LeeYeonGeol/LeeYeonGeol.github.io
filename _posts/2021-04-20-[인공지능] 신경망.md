---
title: "[인공지능] 신경망"
categories: 
- 인공지능
tags:
- 딥러닝
toc: true   #Table Of Contents 목차 
use_math: true
toc_sticky: true
---

# 개요

인공 신경망(artificial neural network)은 생물학적 신경망의 중추신경계인 인간의 뇌가 문제를 처리하는 방식을 모방한 모형으로, 기계학습과 인지과학에서 아주 많이 활용되고 있다.

## 신경망의 특성

- 신경망의 각 노드에 미분 가능한 비선형 활성함수를 적용한다.
- 신경망은 하나 이상의 은닉층을 포함한다.
- 신경망은 높은 수준의 연결성을 나타낸다. 이때 연결강도는 신경망의 가중치에 의해 결정된다.

# 신경망의 기본개념

1. 생물학적 뉴런

일반적으로 뉴런은 다음과 같은 부분으로 구성되어 있다.

- 세포체: 뉴런의 구조적 생화학적 중심
- 가지돌기: 전기화학신호를 받아들이는 나뭇가지 모양의 짧은 돌기
- 축삭: 활동전위를 다른 뉴런에 전달하는, 세포체에서 길게 뻗어 나온 가지
- 시냅스: 한 뉴런에서 다른 뉴런으로 신호를 전달하는 연결부분

뉴런은 세포체와 가지돌기를 통해 여러 가지 흥분성 및 억제성 입력을 수용하며, 세포체는 결합된 입력에 일종의 전달함수를 적용하고 그 결과를 축삭을 통해 펄스 신호 형태로 다른 뉴런과 근육에 보낸다.

2. 인공 뉴런

1943년 매컬로크와 피츠는 생물학적 뉴런의 정보처리 기능을 추상화하여 수십년 동안 중요하게 다루어진 간단한 신경망 모형을 개발하였다. McCulloch-Pitts 모형은 $d$개의 입력 $x_1, x_2,\cdots , x_d $에 대한 출력을 $o$라고 할 때 다음과 같이 일반화된다.

> $o = f(\sum_{i = 1}^{d}{W_{i}x_i+b})$

여기서 $x_i$는 입력층의 $i$번째 노드의 입력, $W_i$는 관련 가중치, $b$는 편향 또는 상수항, $f$는 활성함수(전달함수), $o$는 출력층 노드의 출력을 나타낸다.

다음은 일반화된 McCulloch-Pitts 뉴런의 다이어그램을 보여 준다.

<p align = "center"><img src="https://user-images.githubusercontent.com/48538655/115283990-129f0380-a187-11eb-84ea-58f4c24c6155.png" alt="image" style="zoom:80%;" />
</p>

활성함수 $f$는 어떤 함수든 가능하다. 예를 들어, 계단함수, 시그모이드 함수, 쌍곡탄젠트 함수 및 ReLU 함수를 사용할 수 있다.

McCulloch-pitts 노드가 적용하는 활성함수는 계단함수이다. 대표적인 활성함수는 시그모이드 함수 $f(x) = 1/(1+e^{-x})$이며, 쌍곡탄젠트 함수 $f(x) = tanh(x) = sinhx/coshx = (e^x-e^{-x})/(e^x+e^{-x})$도 많이 사용된다. 이 함수들은 미분 가능하므로 역전파 알고리즘을 이용하여 신경망을 훌련할 때 쉽게 사용할 수 있다.

시그모이드 함수는 매우 높거나 매우 낮은 입력값이 출력에 아주 적은 영향을 주도록 만드는 압축 효과가 있어서 함수의 출력값은 근사적으로 범위 (0,1)로 제한된다. ReLU 함수는 시그모이드 함수의 단점을 보완하기 위해 직선 좋바을 통해 비선형성을 표현하는 함수로, 딥러닝을 위해 많이 사용된다.

# 신경망 작동원리

일반적으로 신경망은 완전히 연결되어 있다. 즉, 모든 층의 노드가 이전 층의 모든 노드에 연결되어 있음을 의미한다.

## 함수신호와 오차신호

신경망에는 두 가지 기본신호가 있다.

- 함수신호: 신경망의 입력노드로 들어와 신경망을 통해 노드별로 전방향으로 전파되어 신경망의 출력노드에서 출력신호로 나오는 신호이다. 각 노드에서 신호는 해당 노드에 적용되는 입력과 연관된 가중치의 함수로 계산된다.
- 오차신호: 신경망의 출력노드에서 시작하여 신경망을 통해 층별로 역방향으로 전파된다. 

## 배치학습과 온라인학습, 미니배치학습

1. 배치학습

배치학습에서는 훈련 데이터를 구성하는 관측값 $N$개가 모두 투입된 후에 가중치의 갱신이 이루어진다. 이대 관측값 $N$개가 한차례 모두 투입되는 것을 1회 에포크로 정의한다.

즉, 배치학습을 위한 비용함수는 평균오차함수 $E_{av}(N)$이며, 가중치의 갱신은 에포크마다 한 번씩 이루어진다. 따라서 에포크마다 가중치의 갱신이 끝나면 $E_{av}(N)$을 계산하여 미리 정한 최대 에포크까지 학습곡선을 그릴 수 있다.

>경사 하강법의 장점
>
>- 가중치 벡터 $w$에 대한 비용함수 $E_{av}(N)$의 미분을 정확하게 추정함으로써 간단한 조건에서 최대경사법의 극솟값으로의 수렴 보장
>- 학습과정의 병렬화

실용적인 관점에서 배치학습은 컴퓨터 저장공간을 다소 많이 차지하므로 훈련 데이터가 비교적 적을 때 가장 유용하다.

2. 온라인학습

온라인학습에서 가중치의 갱신은 관측값별로 수행된다. 즉, 온라인학습에서 최소화되는 함수는 총순간오차 $E(n)$이며, 새로운 관측값이 주어질 때마다 가중치의 갱신이 이루어진다. 온라인학습은 관측값을 하나씩 투입하며 중지규칙이 충족될 때까지 가중치를 갱신한다. 모든 관측값이 한 번 사용되고 중지규칙이 충족되지 않으면, 관측값을 다시 투입하여 가중치 갱신을 계속 한다. 따라서 훈련 데이터가 클 때 배치학습보다 온라인학습이 더 유용하다.

학습할 때마다 가중치를 갱신하므로 신경망 성능이 들쑥날쑥 변하르모 확률적 경사하강법이라고 한다.

>온라인학습의 장점
>
>- 확률적 경사하강법으로 볼 수 있어서 학습과정이 극솟값에 빠지는 것을 막아준다.
>- 배치학습보다 컴퓨터 저장곤간을 훨씬 더 적게 처리한다.

3. 미니배치학습

미니배치학습에서는 훈련 데이터의 관측값을 비슷한 크기의 작은 조각 여러 개인 미니배치로 나눈 후, 하나의 미니배치가 주어질 때마다 경사 하강법을 이용하여 가중치의 갱신이 이루어진다.

또한, 미니배치학습은 배치학습과 온라인학습을 절충한 것으로 대용량 훈련 데이터를 학습할 때 가장 적합할 수 있다.

# 역전파 알고리즘

역전파 알고리즘은 미분의 연쇄법칙을 반복적으로 적용하는 비선형 최소평균제곱 알고리즘의 일종이다. 이 알고리즘은 퍼셉트론 모형의 한계점을 극복하였다. 기본적으로 두 가지 종류가 있는데,

1. 평균오차제곱합 손실함수로 유도한 델타규칙에 기반한 알고리즘
2. 교차엔트로피 손실함수로 유도한 델타규칙에 기반한 알고리즘

1번의 경우, 분류 문제와 회귀분석 문제를 다룰 때 모두 사용하지만, 일반적으로는 회귀분석 문제에 더 많이 사용한다.

2번의 경우, 분류 문제를 다룰 때 주로 사용한다.

## 역전파 알고리즘 학습과정 (온라인 학습)

1. 신경망 모형의 고주, 학습률 및 중지규칙 등을 결정하고 가중치를 초기화한다.
2. 훈련 데이터의 $N$개 관측값을 어떤 형식으로 정렬한 후 신경망에 제시한다.
3. 전방향 계산

> $z^l_j(n) = \sum_{i = 1}^{m_i}{W^l_{ij}h^{l-1}_i(n)}$
> $h^l_j(n) = f_j(z^l_j(n)) $
> $e_j(n) = y_j(n) - o_j(n)$

4. 역방향 계산 (출력층, 은닉층 순)

> $\delta^L_j(n) = -e^L_j(n)f'_j(z^L_j(n))$
>
> $\delta^l_j(n) = -f'_j(z^l_j(n))\sum_k {\delta^{l+1}_j(n)W^{l+1}_{jk}(n)}$

5. 각 관측값에 대해 다음 델타규칙에 따라 가중치를 갱신한다.

> $W^l_{ij}(n+1) = W^l_{ij}(n) + \eta\; \delta^l_j(n)h^{l-1}_i(n)$

6. 중지규칙이 충족될 때까지 단계 2부터 단계 5까지 반복한다.

## 역전파 알고리즘의 변형

1. BFGS

경사 하강법의 변형으로, 손실함수의 2차 미분을 구하여 일반적 경사 하강법의 몇 가지 한계를 극복한다.

2. AdaGrad

역전파 알고리즘의 고정된 학습률을 해결하기 위해 가중치마다 학습률을 다르게 설정해 가중치를 갱신하는 기법이다. 가중치마다 맞춤형 값을 만들어 주며, 적응적으로 학습률을 조정하면서 진행한다.

3. RMSProp

AdaGrad의 학습이 오래 진행될 경우 가중치의 갱신이 이루어지지 않는 경향을 보완하기 위해 제안된 기법이다. $r_{ij}(n)$을 구하는 과정에서 제곱합 대신에 지수평균으로 변경한다.

4. Adam

가중치마다 학습률을 다르게 설정+모멘텀 개념을 융합한 기법이다.

5. Rprop 및iRprop

오차함수의 기울기 크기가 아닌 부호만을 활용하여 학습이 진행된다.



# 참고자료

텐서플로로 배우는 딥러닝 (박혜정, 석경하, 심주용, 황창하 지음)

