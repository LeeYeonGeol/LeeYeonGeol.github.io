---
title: "[인공지능] 경사 하강법의 이해"
categories: 
- 인공지능
tags:
- 딥러닝
- 경사 하강법
toc: true   #Table Of Contents 목차 
use_math: true
toc_sticky: true
---

## 비용 최소화 하기 - 경사 하강법(Gradient Descent)

W 파라미터의 개수가 적다면 고차원 방정식으로 비용 함수가 최소가 되는 W 변숫값을 도출할 수 있지만, W 파라미터가 많으면 해결하기 쉽지 않다. 경사 하강법이 이를 해결해주면서 비용 함수 RSS를 최소화하는 방법을 직관적으로 제공한다.

> 많은 W 파라미터가 있는 경우에 경사 하강법은 보다 간단하고 직관적인 비용함수 최소화 솔루션을 제공

경사 하강법의 사전적 의미인 '점진적인 하강'이라는 뜻에서도 알 수 있듯이, **'점진적으로' 반복적인 계산을 통해 W 파라미터 값을 업데이트하면서 오류 값이 최소가 되는 W 파라미터를 구하는 방식**이다.

- 비용 함수의 반환 값, 즉 예측값과 실제 값의 차이가 작아지는 방향성을 가지고 W 파라미터를 지속해서 보정해 나간다.
- 오류를 감소시키는 방향으로 W 값을 계속 업데이트한다.
- 그리고 오류 값이 더 이상 작아지지 않으면 그 오류 값을 최소 비용으로 판단하고 그때의 W 값을 최적 파라미터로 반환한다.

> 경사하강법의 핵심은 **"어떻게 하면 오류가 작아지는 방향으로 W 값을 보정할 수 있는가?"**이다.

## 미분을 통해 비용 함수의 최소값을 찾기

경사 하강법은 최초 W에서부터 미분을 적용한 뒤 이 미분 값이 계속 감소하는 방향으로 순차적으로 W를 업데이트하고 미분된 1차 함수의 기울기가 감소하지 않는 지점을 비용 함수가 최소인 지점으로 간주하고 그때의 W를 반환한다.

## 손실 함수의 편미분

> Loss(w)는 MSE이고 $Loss(w) = \frac{1}{N} \sum_{i=1}^{N}(y_i-(w_0+w_1*x_i))^2$이다.

- MSE Loss 함수는 변수가 w 파라미터로 이루어진 함수이다.
- Loss(w)를 미분해 미분 함수의 최솟값을 구해야 하는데, Loss(w)는 두 개의 w 파라미터인 $w_0$와 $w_1$을 각각 가지고 있기 때문에 일반적인 미분을 적용할 수가 없고, $w_0$, $w_1$ 각 변수에 편미분을 적용해야 한다.
- Loss(w)를 최소화하는 $w_0$와 $w_1$의 값은 각각  $r(w)$를 $w_0$, $w_1$으로 순차적으로 편미분을 수행해 얻을 수 있다.

>  $\frac{dLoss(w)}{dw_1} = \frac{2}{N}\sum_{i=1}^{N}-x_i*(y_i-(w_0+w_1x_i)) = -\frac{2}{N}\sum_{i=1}^{N}x_i *(실제값_i - 예측값_i)$
>
>  $\frac{dLoss(w)}{dw_0} = \frac{2}{N}\sum_{i=1}^{N}-(y_i-(w_0+w_1x_i)) = -\frac{2}{N}\sum_{i=1}^{N}(실제값_i - 예측값_i)$

## 가중치(Weight), 절편(Bias) 업데이트 하기

가중치/절편 W 값은 손실 함수의 편미분 값을 Update 하면서 계속 갱신한다.

업데이트 과정에서 우리의 목표는 $w_{new} = w_{old}+이동거리*기울기의 부호$가 된다.

-  기울기의 부호의 경우에는 기울기가 양수인경우는 -방향으로 음수인경우는 +방향으로 움직여야되므로 -가 나온다. 
- 이동거리의 경우 최소값을 찾기 위해서는 값이 큰 경우에는 크게, 값이 작아질 수록 적게 움직여야 발산하지 않는데 기울기의 특성이 0으로 갈수록 조금씩 움직이는 특성을 갖기 때문에 기울기를 그대로 사용해도 무방하다. 

Update는 기존 W값에 손실 함수 편미분 값을 감소 시키는 방식을 적용하되 편미분 값을 그냥 감소 시키지 않고 일정한 계수를 곱해서 감소 시키며, 이를 학습률 이라고 한다.

>$w_{1,new} = w_{1,old} - \eta \frac{dLoss(w)}{dw_1} =  w_{1,old} + \eta (\frac{2}{N}\sum_{i=1}^{N}x_i *(실제값_i - 예측값_i))$
>
>$w_{0,new} = w_{0,old} - \eta \frac{dLoss(w)}{dw_0} =  w_{0,old} + \eta (\frac{2}{N}\sum_{i=1}^{N}(실제값_i - 예측값_i))$

## 프로그램 코드 예제

```python
x_old = 0
x_new = 6 # The algorithm starts at x=6
eps = 0.01 # step size
precision = 0.00001

def f_prime(x):
    return 4 * x**3 - 9 * x**2

while abs(x_new - x_old) > precision:
    x_old = x_new
    x_new = x_old - eps * f_prime(x_old)

print("Local minimum occurs at: " + str(x_new))
```

```
Local minimum occurs at: 2.2499646074278457
```

$f(x)=x^4-3x^3+2$함수의 극값을 미분값인 $f(x)=4x^3-9x^2$을 통해 찾는 예이다.

(출처 - [위키피디아](https://ko.wikipedia.org/wiki/%EA%B2%BD%EC%82%AC_%ED%95%98%EA%B0%95%EB%B2%95))

## 참고 자료

[딥러닝 CNN 완벽가이드](https://www.inflearn.com/course/%EB%94%A5%EB%9F%AC%EB%8B%9D-cnn-%EC%99%84%EB%B2%BD-%EA%B8%B0%EC%B4%88)

[경사하강법 소개](https://www.youtube.com/watch?v=ENnLNkBghhE)