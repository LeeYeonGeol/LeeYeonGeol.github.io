---
title: "[딥러닝] 손실 함수와 크로스 엔트로피"
categories: 
- 딥러닝
tags:
- 손실 함수
- 크로스 엔트로피
toc: true   #Table Of Contents 목차 
use_math: true
toc_sticky: true
---

## 손실/비용/목적 함수

- 손실 함수 = 비용 함수 = 목적 함수
- loss function = cost function = objective function

## 손실 함수(Loss Function) 개요

모델이 얼마나 학습 데이터에 잘 맞고 있는가?

손실 함수의 역할: 학습 과정이 올바르게 이뤄질 수 있도록 적절한 가이드를 제공할 수 있어야 한다.

회귀는 주로 **MSE**를 사용하고 분류는 주로 **Cross Entropy**를 사용한다.

## Softmax/Sigmoid와 Cross Entropy Loss

### Softmax CE Loss (Categorical CE Loss)

실제 클래스 값에 해당하는 Softmax의 결과 값에만 Loss를 부여한다.

아주 잘못된 예측 결과에는 매우 높은 Loss를 부여한다.

> $L = - \frac{1}{m}\sum_{i=1}^{m}y_i\cdot log(\hat y_i)$
>
> $y_i$는 i번째 클래스 실제값, $\hat y_i$는 i번째 클래스 예측값

### Binary CE Loss

> $L = - \frac{1}{m}\sum_{i=1}^{m} (y_i\cdot log(\hat y_i)+(1 - y_i)\cdot log(1-\hat y_i))$

## Cross Entropy와 Squared Error 비교

1. Squared ERROR 기반은 일반적으로 잘못된 예측에 대해서 상대적으로 CE보다 높은 비율의 페널티가 부여되어 Loss값의 변화가 상대적으로 심함. 이 때문에 CE에 비해 최적 수렴이 어려움.
2. 아주 잘못된 예측에 대해서는 CE보다 낮은 비율의 페널티가 부여됨.



## 참고 자료

[딥러닝 CNN 완벽가이드](https://www.inflearn.com/course/%EB%94%A5%EB%9F%AC%EB%8B%9D-cnn-%EC%99%84%EB%B2%BD-%EA%B8%B0%EC%B4%88)

