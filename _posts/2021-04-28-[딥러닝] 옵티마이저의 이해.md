---
title: "[딥러닝] 옵티마이저의 이해"
categories: 
- 딥러닝
tags:
- 옵티마이저
toc: true   #Table Of Contents 목차 
use_math: true
toc_sticky: true
---

## Optimizer

- 보다 최적으로 GD를 적용
- 최소 Loss로 보다 빠르고 안정적으로 수렴할 수 있는 기법 적용 필요

### 주요 메커니즘

기어를 바꾸고, 엑셀을 밟는다.

### 주요 Optimizer

Momentum, AdaGrad (Adaptive Gradient) -> RMSProp -> ADAM (Adaptive Moment Estimation)

### Momentum

가중치를 계속 Update 수행 시마다 이전의 Gradient들의 값을 일정 수준 반영 시키면서 신규 가중치로 Update 적용.

> $w_{t+1} = w_t - v_t$
>
> $v_t = \gamma v_{t-1} + \eta \frac{dLoss}{dw_t}$

#### Momentum의 효과

SGD의 경우는 random한 데이터를 기반으로 Gradient를 계산하므로 지그재그 형태의 Update가 발생하기 쉽다.

Momentum을 통해서 이러한 지그재그 형태의 Update를 일정수준 개선 가능하다.

### AdaGrad(Adaptive Gradient)

가중치 별로 서로 다른 Learning Rate를 동적으로 적용한다. 그동안 적게 변화된 가중치는 보다 큰 Learning Rate를 적용하고, 많이 변화된 가중치는 보다 작은 Learning Rate를 적용한다.

> $g_t = \frac{dLoss}{dw_t}$로 일반화시킨다면
>
> $s_t = s_{t-1}+g_t^2$ 
>
> $w_{t+1} =w_t - \frac{\eta}{\sqrt{s_t +\epsilon}}*g_t$ ($s_t$: 그동안 적용된 Gradient의 제곱 합)

처음에는 큰 Learning Rate가 적용되지만 최저점에 가까울 수록 Learning Rate가 작아진다.

Gradient의 제곱은 언제나 양이기 때문에 iteration시다마 $s_t$값이 증가하면서 Learning Rate값이 **아주 작게** 변환되는 문제점 발생

### RMSProp

지나치게 Learning Rate가 작아지는 것을 막기 위해 지수 가중 평균법으로 구함. (오랜 과거의 Gradient값의 영향을 줄일 수 있도록 한다.)

> $s_t = \gamma s_{t-1}+(1-\gamma)g_t^2$   ($s_t$: 그동안 적용된 Gradient의 제곱 합)

###  Adam

RMSProp과 같이 개별 Weight에 서로 다른 Learning Rate를 적용함과 동시에 Gradient에 Momentum 효과를 부여

> $s_t = \beta_2s_{t-1} + (1-\beta_2)g_t^2$
>
> $v_t = \beta_1v_{t-1} + (1-\beta_1)g_t$
>
> $w_{t+1} = w_t - \frac{\eta}{\sqrt{s_t+\epsilon}}*v_t$
>
> $\beta_2$: 학습률을 위한 지수 가중 평균 계수
>
> $\beta_1$: GD를 위한 지수 가중 평균 계수

## Keras SGD, RMSprop, Adam

```python
keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)
keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)
keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)

model.compile(optimizer=Adam(lr=0.001)),,,,,
```

## 학습률 최적화 유형

- optimizer 방식: weight update 시에 학습률을 동적 변경
- Learning Rate Scheduler 방식: epochs 시마다 성능 평가 지표 등에 따라 동적으로 학습률을 변경하는 방식

## 참고 자료

[딥러닝 CNN 완벽가이드](https://www.inflearn.com/course/%EB%94%A5%EB%9F%AC%EB%8B%9D-cnn-%EC%99%84%EB%B2%BD-%EA%B8%B0%EC%B4%88)

