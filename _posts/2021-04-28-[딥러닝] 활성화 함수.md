---
title: "[딥러닝] 활성화 함수"
categories: 
- 딥러닝
tags:
- 활성화 함수
toc: true   #Table Of Contents 목차 
use_math: true
toc_sticky: true
---

## 활성화 함수(Activation Function)

- Sigmoid Funtion
  - $g(z) = \frac{1}{1+e^{-z}}$
  - $g'(z) = g(z)(1-g(z))$
- Hyperbolic Tangent
  - $g(z) = \frac{e^z-e^{-z}}{e^z+e^{-z}}$
  - $g'(z) = 1- g(z)^2$
- Rectified Linear Unit (ReLU)
  - $g(z) = max(0, z)$
  - $g'(z) = 1$, z > 0
  - ​                0,  otherwise

## 활성화 함수(Activations Function)의 주요 목적

활성화 함수의 주요 목적은 딥러닝 네트워크에 비선형성을 적용하기 위함

![image](https://user-images.githubusercontent.com/48538655/116298526-7eace780-a7d7-11eb-8844-999037d5f8ef.png)

## 딥러닝 모델에서 활성화 함수의 적용

ReLU: 은닉층에 사용됨

Sigmoid: 이진 분류 시 마지막 Classfication 출력층에 사용

Softmax: 멀티 분류 시 마지막 Classification 출력층에 사용

## Sigmoid 함수 특성

시그모이드는 은닉층의 활성화 함수로는 Vanishing Gradient 등의 이슈로 더 이상 사용되지 않는다. 또한 평균이 0이 아니므로 Gradient Update가 지그재그 형태로 발생한다.

0 또는 1값을 반환하는 시그모이드의 특성으로 인해 이진 분류의 확률 값을 기반으로 최종 분류 예측을 적용하는데 시그모이드가 사용된다.

## Hyperbolic Tangent 특성

Tanh는 시그모이드와 달리 -1과 1사이의 값을 출력하여 평균이 0이 될 수 있지만 여전히 입력값이 양(또는 음)으로 큰 값이 입력될 경우에는 출력값의 변화가 미비함

## ReLU(Rectified Linear Unit)

대표적인 은닉층의 활성함수(Activation Function).

입력값이 0보다 작을 떄 출력은 0, 0보다 크면 입력값을 출력

다양한 유형의 변형이 존재한다. (Leaky ReLU, ELU 등)

## 소프트맥스 함수 (Softmax activation)

Sigmoid와 유사하게 Score값을 확률값 0 ~ 1로 변환 하지만, 차이점은 소프트 맥스는 개별 출력값의 총 합이 1이 되도록 매핑해주는 것이다.

여러개의 타겟값을 분류하는 Multi Classification의 최종 활성화 함수로 사용된다.

## 참고 자료

[딥러닝 CNN 완벽가이드](https://www.inflearn.com/course/%EB%94%A5%EB%9F%AC%EB%8B%9D-cnn-%EC%99%84%EB%B2%BD-%EA%B8%B0%EC%B4%88)

