---
title: "[CNN] 배치 정규화의 이해"
categories: 
- CNN
toc: true   #Table Of Contents 목차 
use_math: true
toc_sticky: true
---

## Feature Scaling

서로 다른 Feature 값들을 동일한 척도로 변환

### Scaling 기법

- Min - Max Scaling
  - 0 에서 1 사이 값으로 변환
  - $\frac{(X-최소값)}{(최대값 - 최소값)}$
- Z Score 정규화
  - 평균이 0이고 표준 편차가 1인 데이터 세트로 변환
  - $\frac {x - 평균}{표준편차}$

## BN의 필요성 - Internal Covariate Shift

신경망 내부의 각 층을 통과 할 때마다 입력 데이터의 분포가 조금씩 변경되는 현상.

## Batch Normalization 구성

Conv 적용 후 적용하며, 이후에 Activation을 적용한다.

## Batch Normalization 효과

1. 뛰어난 성능 향상 효과
2. Regularization 효과
   - 정규화 후에 스케일링, Shift 로 일종의 Noise 추가 효과
3. 가중치 초기화(Weight Initialization) 설정을 크게 신경 쓸 필요 없음



## 참고 자료

[딥러닝 CNN 완벽가이드](https://www.inflearn.com/course/%EB%94%A5%EB%9F%AC%EB%8B%9D-cnn-%EC%99%84%EB%B2%BD-%EA%B8%B0%EC%B4%88)

