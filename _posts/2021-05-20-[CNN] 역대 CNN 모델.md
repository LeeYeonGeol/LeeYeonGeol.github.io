---
title: "[CNN] 역대 CNN 모델"
categories: 
- CNN
toc: true   #Table Of Contents 목차 
use_math: true
toc_sticky: true
---

## CNN 아키텍처 특징

- Feature Extrator와 Classification Layer 영역으로 구성
- 연속적인 CNN 연산을 순차적으로 수행하면서 일련의 Feature Map을 생성
- 순차적으로 생성된 Feature map의 크기(높이x너비)는 줄어들지만 채널(깊이)은 증가
- 더 깊은 영역에 있는 Network이 더 복잡한 Feature 정보를 반영하면서 CNN 깊이를 증가시키는 방향으로 아키텍처를 발전 시킴

 ## AlexNet 개요 및 특징

- Activation 함수로 ReLU 함수를 첫 사용
- MaxPooling으로 Pooling 적용 및 Overlapping Pooling 적용
- Local Response Normalization(LRN) 사용
- Overfitting을 개선하기 위해서 Drop out Layer와 Weight의 Decay 기법 적용
- Data Augmentation 적용(좌우 반전, Crop, PCA 변환 등)

##  VGGNet

- 네트워크의 깊이와 모델 성능 영향에 집중
- Convolution 커널 사이즈를 3 x 3으로 고정. 커널 사이즈가 크면 이미지 사이즈 축소가 급격하게 이뤄져서 더 깊은 층을 만들기 어렵고, 파라미터 개수와 연산량도 더 많이 필요.

## GoogLeNet(with Inception module)

- 큰 사이즈의 Receptive Field가 제공하는 장점은 수용하면서 Weight 파라미터의 수를 줄일 수 있는 네트워크 구성
- 적절한 커널의 필터 사이즈와 Pooling을 고민해서 찾아내기 보다는 **여러가지 사이즈의 필터들을 한꺼번에 결합하는 방식 제공**(논문에서는 이를 Inception module로 지칭)
- Inception module들을 연속적으로 이어서 구성
- 여러 사이즈의 필터들이 서로 다른 공간 기반으로 Feature들을 추출하고 이를 결합하면서 보다 풍부한 Feature Extractor layer구성이 가능함

### 1 x 1 Convolution 

- Feature map의 채널수를 감소시키되 비선형성 증대
- 연산량과 파라미터 수 감소

## ResNet

> 깊은 Network 구성 시 주요 문제점
>
> - Vanishing Gradient 문제
> - 제대로 최적 loss 감소가 되지 않음

### ResNet의 주요 특징 -  Short Cut, Identity block

- 이전 Layer의 출력값을 Conv Layer로 거치지 않고 그대로 전달하는 **Shortcut**을 구성

- 기존에는 H(x) - Y가 최소값이 되는 방향으로 학습을 진행하는 반면에 ResNet은 F(x) = H(x) - x를 최소화할 수 있도록 구성 (Residual Block)
-  F(x)가 0이 되도록 학습, F(x) + x = H(x)이므로 미분해도 최소 1로 Vanishing Gradient 현상 극복 가능
- 학습하면서 기존 CNN과 유사하게 입력값의 비선형 적인 특성을 학습. 0으로 최적 수렴하는 것이 깊은 네트워크에서 최소 Loss 최적화를 더 빠르고 효율적으로 수행 가능

## EfficientNet

네트워크의 깊이, 필터 수, 이미지 Resolution크기를 **최적으로 조합하여** 모델 성능을 극대화함

### Compound Scaling 적용

- depth와 resolution을 각각 1.0으로 고정하고 width만 증가 시켰을 때 정확도 성능은 80%에서 수렴
- depth를 2.0, resolution은 2.0으로 했을 경우 width만 변화 시키면 **비슷한 FLOPS상에서** 더 나은 성능을 보임

 기존 MobileNet과 ResNet에 Compound Scaling 적용.

## 참고 자료

[딥러닝 CNN 완벽가이드](https://www.inflearn.com/course/%EB%94%A5%EB%9F%AC%EB%8B%9D-cnn-%EC%99%84%EB%B2%BD-%EA%B8%B0%EC%B4%88)

